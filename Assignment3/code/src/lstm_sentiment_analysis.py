# -*- coding: utf-8 -*-
"""lstm_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13If5RuhNy2P1mBLiQK7SgYsPZTdECMzN
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/CS6910_PA3/
!pwd

"""## Tokenize samples"""

import torch
from torchtext import data
import spacy
import nltk
from nltk.corpus import stopwords
import string
from tqdm import tqdm
nltk.download('stopwords')
stop = stopwords.words('english') + list(string.punctuation)
nlp = spacy.load("en_core_web_sm")

"""## Preprocess and load data"""

#remove stop words from text data
def cleanup_text(corpus):
  cleaned_corpus = [w for w in corpus if w not in stop]
  #no empty sequences
  if not cleaned_corpus:
    cleaned_corpus = corpus
  return cleaned_corpus
#tokenize sentence using spacy
def my_tokenizer(text):
  return [token.text for token in nlp.tokenizer(text)]

TEXT = data.Field(preprocessing=cleanup_text, tokenize=my_tokenizer,batch_first=True, include_lengths=True)
LABEL = data.LabelField(dtype=torch.long, batch_first=True)

fields = [('PhraseId',None),
          ('SentenceId',None),
          ('Phrase',TEXT),
          ('Sentiment',LABEL)]

train_data, val_data, test_data = data.TabularDataset.splits(
    path='data/',
    train = 'train.tsv',
    validation = 'val.tsv',
    test = 'test.tsv',
    format = 'tsv',
    fields = fields,
    skip_header = True
)

BATCH_SIZE=64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

trainloader, valloader, testloader = data.BucketIterator.splits(
    (train_data, val_data, test_data),
    sort_key=lambda x: len(x.Phrase),
    shuffle=True,
    sort_within_batch=False,
    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),
    device=device)

"""## Define LSTM model"""

import torch.nn as nn
import torch.nn.functional as F

class SentimentLSTM(nn.Module):
  def __init__(self, vocab_size, embedding_dim, hidden_size, n_classes):
    super(SentimentLSTM, self).__init__()

    self.vocab_size = vocab_size
    self.embedding_dim = embedding_dim
    self.hidden_size = hidden_size
    self.n_classes = n_classes

    #non trainable embedding layer
    self.embed = nn.Embedding(vocab_size, embedding_dim)
    self.embed.requires_grad=False

    #lstm layer
    self.lstm = nn.LSTM(input_size=self.embedding_dim, 
                        hidden_size=self.hidden_size, 
                        batch_first=True)
    
    #final fc layer
    self.fc = nn.Linear(self.hidden_size, self.n_classes)
  
  def forward(self, x, seq_lengths):
    #[batch_size, seq_lengths]
    # print("INPUT SHAPE "+ str(x.shape))
    embedding = self.embed(x)
    # print("EMBEDDING SHAPE "+str(embedding.shape))
    #print(seq_lengths.to('cpu'))
    #[batch_size, seq_lengths, embedding_dim]
    packed_embedding = nn.utils.rnn.pack_padded_sequence(embedding, 
                                                         lengths = seq_lengths.cpu(),
                                                         batch_first=True, 
                                                         enforce_sorted=False)

    packed_output, (h_n, c_n) = self.lstm(packed_embedding)
    # print("h_n shape "+str(h_n.shape))
    #[batch_size, 1, hidden_size] to [batch_size, hidden_size]      
    out = self.fc(h_n.squeeze(0))
    # print("OUTPUT SHAPE "+str(out.shape))
    return out

"""## Training functions"""

import os
import torch.optim as optim
import torch.nn as nn
import numpy as np

#train for 1 epoch
def train(epoch, trainloader, net, optimizer, criterion):
  
  run_loss=0
  total=0
  correct=0

  for batch_idx, data in enumerate(trainloader):
    optimizer.zero_grad()
    text_val, text_len = data.Phrase
    #print(text_val)
    #print(text_len)
    
    outputs = net(text_val, text_len)
    
    loss = criterion(outputs, data.Sentiment)
    # print('hehe!')
    # break
    loss.backward()
    optimizer.step()
    
    run_loss += loss.item()
    _, predicted = torch.max(outputs.data, 1)
    total += data.Sentiment.size(0)
    correct += (predicted == data.Sentiment).sum().item()
    if(batch_idx%200==0):
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.
              format(epoch+1, batch_idx, len(trainloader),
                       100. * batch_idx / len(trainloader), loss.item()))
  
  run_loss = run_loss / (len(trainloader))
  run_acc = (100 * correct / total)
  print('Epoch %d training loss: %.6f, acc: %.3f' %
          (epoch + 1, run_loss, run_acc))
  
  return run_loss, run_acc

#perform validation for 1 epoch
def test(testloader, net, criterion):
  run_loss=0.0
  total=0
  correct=0
  with torch.no_grad():
    for data in testloader:
      text_val, text_len = data.Phrase
      outputs = net(text_val, text_len)
      loss = criterion(outputs, data.Sentiment)

      run_loss += loss.item()
      _, predicted = torch.max(outputs.data, 1)
      total += data.Sentiment.size(0)
      correct += (predicted == data.Sentiment).sum().item()

  run_loss = run_loss / (len(testloader))
  run_acc = (100 * correct / total)
  print('Test loss: %.6f, acc: %.3f' %
          (run_loss, run_acc))
  return run_loss, run_acc

#train current model
def train_model(model_name, net, num_epochs, optimizer, criterion):

  print("Started Training %s" % (model_name))
  
  num_params = np.sum([p.nelement() for p in net.parameters()])
  print(num_params, ' parameters')
  
  tr_loss = []
  tr_acc = []
  val_loss = []
  val_acc = []
  best_valid_loss = float('inf')

  for epoch in range(num_epochs):  # loop over the dataset multiple times
    print('epoch ', epoch + 1)
    epoch_loss, epoch_acc = train(epoch, trainloader, net, optimizer, criterion)
    tr_loss.append(epoch_loss)
    tr_acc.append(epoch_acc)
    
    net.eval()
    
    epoch_loss, epoch_acc = test(valloader, net, criterion)
    val_loss.append(epoch_loss)
    val_acc.append(epoch_acc)

    net.train()

    #save only the best model seen so far
    model_path = './models/'+str(model_name)+'.pth'
    if(val_loss[-1] < best_valid_loss):
      print("!Updating best model to epoch %d"%(epoch+1))
      best_valid_loss = val_loss[-1]
      torch.save({'epoch':epoch+1,
                  'model_state_dict':net.state_dict()}, model_path)
      
  print('Performing Test')
  test(testloader, net, criterion)

  print('Finished Training %s' % (model_name))
  return tr_loss, tr_acc, val_loss, val_acc

"""## Train Models per embedding"""

tqdm._instances.clear()

os.makedirs('./models',exist_ok=True)
criterion = nn.CrossEntropyLoss()
criterion.to(device)
NUM_EPOCHS = 10
LEARNING_RATE = 0.01

from torchtext.vocab import Vectors
from torchtext.vocab import GloVe

"""GloVe embedding_dim=50"""

vectors = GloVe(name='6B',dim=50)
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss, tr_acc, val_loss, val_acc = train_model('gloVe50',net,NUM_EPOCHS,optimizer,criterion)

"""GloVe embedding_dim=100"""

vectors = GloVe(name='6B',dim=100)
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss2, tr_acc2, val_loss2, val_acc2 = train_model('gloVe100',net,NUM_EPOCHS,optimizer,criterion)

"""CBOW embedding_dim=100"""

vectors = Vectors(name='./data/cbow_d100.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss3, tr_acc3, val_loss3, val_acc3 = train_model('CBow100',net,NUM_EPOCHS,optimizer,criterion)

"""CBOW embedding_dim=200"""

vectors = Vectors(name='./data/cbow_d200.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss4, tr_acc4, val_loss4, val_acc4 = train_model('CBow200',net,NUM_EPOCHS,optimizer,criterion)

"""Skipgram embedding_dim=100"""

vectors = Vectors(name='./data/skipgram_d100.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss5, tr_acc5, val_loss5, val_acc5 = train_model('SkipGram100',net,NUM_EPOCHS,optimizer,criterion)

"""Skipgram embedding_dim=200"""

vectors = Vectors(name='./data/skipgram_d200.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss6, tr_acc6, val_loss6, val_acc6 = train_model('SkipGram200',net,NUM_EPOCHS,optimizer,criterion)

"""GloVe embedding_dim=200"""

vectors = GloVe(name='6B',dim=200)
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.embed.weight.data.copy_(TEXT.vocab.vectors)
optimizer = optim.SGD(net.parameters(),lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)

net.to(device)
tr_loss7, tr_acc7, val_loss7, val_acc7 = train_model('gloVe200',net,NUM_EPOCHS,optimizer,criterion)

"""## Find Accuracy and Confusion Matrices"""

def classwise_test(net, testloader, n_classes):
  
  conf = torch.zeros(n_classes, n_classes,dtype=torch.long)

  with torch.no_grad():
    for data in testloader:
      text_input, text_len = data.Phrase
      classes = data.Sentiment
      outputs = net(text_input, text_len)
      _, predicted = torch.max(outputs, 1)
      for t,p in zip(classes.view(-1), predicted.view(-1)):
        conf[t.long(),p.long()] += 1
  totals = torch.sum(conf, axis=1)
  for i in range(n_classes):
    print("Class %s : %.3f"%(LABEL.vocab.itos[i],100*conf[i,i]/totals[i]))
  return conf

from torchtext.vocab import GloVe
from torchtext.vocab import Vectors
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import itertools

def save_confusion_matrix(file_name,cm,target_names,title='Confusion Matrix',cmap=None,normalize=True):

  accuracy = np.trace(cm) / float(np.sum(cm))
  misclass = 1 - accuracy

  if cmap is None:
    cmap = plt.get_cmap('Blues')

  plt.figure(figsize=(8, 6))
  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()

  if target_names is not None:
    tick_marks = np.arange(len(target_names))
    plt.xticks(tick_marks, target_names, rotation=45)
    plt.yticks(tick_marks, target_names)

  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


  thresh = cm.max() / 1.5 if normalize else cm.max() / 2
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    if normalize:
      plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                horizontalalignment="center",
                color="black") #if cm[i, j] > thresh else "black")
    else:
      plt.text(j, i, "{:,}".format(cm[i, j]),
                horizontalalignment="center",
                color="black") #if cm[i, j] > thresh else "black")


  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
  plt.savefig(file_name, bbox_inches='tight',format='png')

"""GloVe embedding_dim=50"""

vectors = GloVe(name='6B',dim=50)
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/gloVe50.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('GloVe50.png',conf.numpy(),LABEL.vocab.itos,title='GloVe 50 dim',cmap='Blues')

"""GloVe embedding_dim=100"""

vectors = GloVe(name='6B',dim=100)
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/gloVe100.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('GloVe100.png',conf.numpy(),LABEL.vocab.itos,title='GloVe 100 dim',cmap='Blues')

"""GloVe embedding_dim=200"""

vectors = GloVe(name='6B',dim=200)
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/gloVe200.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('GloVe200.png',conf.numpy(),LABEL.vocab.itos,title='GloVe 200 dim',cmap='Blues')

"""CBow embedding_dim=100"""

vectors = Vectors(name='./data/cbow_d100.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/CBow100.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('CBow100.png',conf.numpy(),LABEL.vocab.itos,title='CBOW 100 dim',cmap='Blues')

"""CBow embedding_dim=200"""

vectors = Vectors(name='./data/cbow_d200.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/CBow200.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('CBow200.png',conf.numpy(),LABEL.vocab.itos,title='CBOW 200 dim',cmap='Blues')

"""SkipGram embedding_dim=100"""

vectors = Vectors(name='./data/skipgram_d100.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/SkipGram100.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('SkipGram100.png',conf.numpy(),LABEL.vocab.itos,title='Skipgram 100 dim',cmap='Blues')

"""SkipGram embedding_dim=200"""

vectors = Vectors(name='./data/skipgram_d200.txt')
TEXT.build_vocab(train_data,val_data, vectors=vectors)
LABEL.build_vocab(train_data)

model_path = './models/SkipGram200.pth'
checkpoint = torch.load(f=model_path, map_location=device)
net = SentimentLSTM(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 5)
net.load_state_dict(checkpoint['model_state_dict'])
net.to(device)
net.eval()

conf = classwise_test(net,testloader,5)
save_confusion_matrix('SkipGram200.png',conf.numpy(),LABEL.vocab.itos,title='Skipgram 200 dim',cmap='Blues')

